{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8262acb6",
   "metadata": {},
   "source": [
    "# Cassandra: Import Large Datasets\n",
    "\n",
    "In this module, we import large datasets in CSV files into Cassandra."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c902f566",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Monthly behavior datasets from multi category store (about 50 Millions of records each month).\n",
    "\n",
    "https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store\n",
    "\n",
    "There are 4 types of logged events:\n",
    "* view - a user viewed a product\n",
    "* cart - a user added a product to shopping cart\n",
    "* remove_from_cart - a user removed a product from shopping cart\n",
    "* purchase - a user purchased a product. A session can have multiple purchase events. It's normal, because it's a single order."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd149a6b",
   "metadata": {},
   "source": [
    "## Cassandra Cluster\n",
    "\n",
    "In a Cassandra cluster, there is no central primary (or master) node. All nodes in the cluster are peers. There are mechanisms, such as the Gossip protocol to determine when the cluster is first started for nodes to discover each other.\n",
    "\n",
    "Once the topology is established, however, it is not static. This same Gossip mechanism helps to determine when additional nodes are added to the cluster, or when nodes are removed from the cluster.\n",
    "\n",
    "### Check Cluster Status\n",
    "\n",
    "You can conduct a Cassandra cluster health check with nodetool status. Nodetool status commands allow you to check Cassandra cluster status and view things like data distribution among nodes, whether nodes are up or down, node states, node data loads, token numbers, and related information.\n",
    "\n",
    "The nodetool info command offers node information, including active or passive gossip status, uptime, disk load, chunk cache information, times started (generation), heap memory usage, and more.\n",
    "\n",
    "Finally, the nodetool tpstats command shows thread pool usage statistics at each stage.\n",
    "\n",
    "```bash\n",
    "$ nodetool status\n",
    "$ nodetool status keyspace.table\n",
    "$ nodetool tablestats\n",
    "```\n",
    "\n",
    "### Setup Cluster with Docker\n",
    "\n",
    "1. Download\n",
    "\n",
    "The official image of a Cassandra worker is available at [Docker Hub](https://hub.docker.com/_/cassandra). \n",
    "\n",
    "```bash\n",
    "$ docker pull cassandra\n",
    "Using default tag: latest\n",
    "latest: Pulling from library/cassandra\n",
    "5544ebdc0c7b: Pull complete\n",
    "9f11d3ecf1bb: Pull complete\n",
    "67ea14bd9996: Pull complete\n",
    "94e309096fff: Pull complete\n",
    "9330c089b83e: Pull complete\n",
    "f3837b8087ee: Pull complete\n",
    "11c520a8aabe: Pull complete\n",
    "ce53e4df33e6: Pull complete\n",
    "f46e33d0cbbe: Pull complete\n",
    "Digest: sha256:e3a505f2cb0f53b730d43ea06235b5fae14bf5006d904a096089baf77dbaf217\n",
    "Status: Downloaded newer image for cassandra:latest\n",
    "docker.io/library/cassandra:latest\n",
    "```\n",
    "\n",
    "2. Basic configurations\n",
    "\n",
    "With `docker run`, we can pre-configure the cluster:\n",
    "\n",
    "* `--name cassandra_node0`: name of the container which is creating\n",
    "* `-d`: Run in the background (detached mode)\n",
    "* `--memory=\"8g\" --cpus=\"2.0\"`: limit the resources used by the container. In this case, 8GB RAM and 2.0 cpus are used (equal to use 2 cpu cores).\n",
    "* `-v E:/shared_data/v0:/var/lib/cassandra`: specify external storage for Cassandra.\n",
    "* `-p 9042:9042 -p 7000:7000 -p 7001:7001 -p 7199:7199`: binding to come common ports using by external and internal to communicate with cluster.\n",
    "* `-e CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch`: specify which Gossip mechanism is used. `GossipingPropertyFileSnitch` is recommended for production use in yaml file.\n",
    "* `-e CASSANDRA_BROADCAST_ADDRESS=192.168.137.101`: Set broadcast IP address. Must set for the main nodes or first node to be turned on.\n",
    "* `-e CASSANDRA_SEEDS=192.168.137.101`: tell other nodes to connect first to this node to get and exchange additional information.\n",
    "* `-e CASANDRA_CLUSTER_NAME=cass_recommender`: Set cluster name\n",
    "* `-e CASSANDRA_DC=dc1` and `-e CASSANDRA_RACK=rack1`: Set Data Center and Rack number of this node\n",
    "\n",
    "Examples:\n",
    "\n",
    "Main node - node0\n",
    "```bash\n",
    "$ docker run --memory=\"8g\" --cpus=\"2.0\" \\\n",
    "    --name cassandra_node0 \\\n",
    "    -v E:/shared_data/v0:/var/lib/cassandra \\\n",
    "    -e CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch \\\n",
    "    -e CASSANDRA_BROADCAST_ADDRESS=192.168.137.101 \\\n",
    "    -e CASANDRA_CLUSTER_NAME=cass_recommender \\\n",
    "    -e CASSANDRA_DC=dc1 \\\n",
    "    -p 9042:9042 -p 7000:7000 -p 7001:7001 -p 7199:7199 \\\n",
    "    -d cassandra:latest\n",
    "```\n",
    "\n",
    "Other nodes - node1\n",
    "\n",
    "```bash\n",
    "$ docker run --memory=\"8g\" --cpus=\"3.0\" \\\n",
    "    --name cassandra_node1 \\\n",
    "    -v E:/shared_data/v1:/var/lib/cassandra  \\\n",
    "    -e CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch \\\n",
    "    -e CASSANDRA_SEEDS=192.168.137.101 \\\n",
    "    -e CASANDRA_CLUSTER_NAME=cass_recommender \\\n",
    "    -e CASSANDRA_DC=dc1 \\\n",
    "    -d cassandra:latest\n",
    "```\n",
    "\n",
    "To get the IP of a running container, we can use `inspect`:\n",
    "```\n",
    "$ docker inspect cassandra_node0\n",
    "```\n",
    "\n",
    "3. External Storage\n",
    "\n",
    "If there was no external storage mounted to a container, we might lost data if the container is broken or deleted. It's also hard to move data around as well as take care of it. Hence, we can create a new volume with Docker using `docker volume create` command, or we can point a local directory and mount it when creating the container.\n",
    "\n",
    "```bash\n",
    "$ docker run -v volume/dir:remote/dir\n",
    "$ docker run -v local/dir:remote/dir\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7239465",
   "metadata": {},
   "source": [
    "3. Run Cluster\n",
    "\n",
    "First, turn on the main node - the node0. Then turn on other nodes. It takes time for all nodes in the cluster to communicate and connect with each other. Once all nodes are connected, we can check status:\n",
    "\n",
    "```\n",
    "$ docker exec -it cassandra_node0 nodetool status\n",
    "Datacenter: dc1\n",
    "===============\n",
    "Status=Up/Down\n",
    "|/ State=Normal/Leaving/Joining/Moving\n",
    "--  Address          Load        Tokens  Owns (effective)  Host ID                               Rack\n",
    "UN  172.17.0.3       117.57 KiB  16      64.7%             d9f529c7-9b81-462e-8ec1-85938fba359d  rack1\n",
    "UN  192.168.137.101  134.89 KiB  16      62.1%             ad7179b0-fd7c-4ce4-8903-4451d1a59af6  rack1\n",
    "\n",
    "Datacenter: dc2\n",
    "===============\n",
    "Status=Up/Down\n",
    "|/ State=Normal/Leaving/Joining/Moving\n",
    "--  Address          Load        Tokens  Owns (effective)  Host ID                               Rack\n",
    "UN  172.17.0.2       105.89 KiB  16      73.2%             8b37d127-3e34-4fe4-b9a8-aa415ee6023a  rack1\n",
    "```\n",
    "\n",
    "To access the cqlsh:\n",
    "\n",
    "```bash\n",
    "$ docker exec -it container_ID bash\n",
    "root@64a8dddfaebf:/#cqlsh\n",
    "```\n",
    "or \n",
    "\n",
    "```bash\n",
    "$ docker exec -it container_ID cqlsh\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f7949e5",
   "metadata": {},
   "source": [
    "### Notes \n",
    "\n",
    "1. External Data Center\n",
    "\n",
    "In case there are external data centers which are outside of the current Docker environment, we need to map port 7000 (-p 7000:7000), assign first node with real IP CASSANDRA_BROADCAST_ADDRESS=192.168.137.101, tell second node about this seed  CASSANDRA_SEEDS=192.168.137.101 (from first node). \n",
    "\n",
    "Note: It's not easy to access internal running containers from outside of Docker, for example, from another computer or new virtual machine. In this case, we only can run one instance of cassandra for each Docker and assign the machine's IP for each container (bound with port 7000)\n",
    "\n",
    "2. Recommended resources\n",
    "\n",
    "A minimal production server requires at least 2 cores, and at least 8GB of RAM. Typical production servers have 8 or more cores and at least 32GB of RAM. \n",
    "\n",
    "We can allocate when creating a new container:\n",
    "```bash\n",
    "$ docker run --memory=\"5g\" --cpus=\"1.0\" IMAGE\n",
    "```\n",
    "\n",
    "Or adjust the current container\n",
    "```bash\n",
    "$ docker container update --memory=\"5g\" --cpus=\"1.0\" CONTAINER\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e330e1a",
   "metadata": {},
   "source": [
    "## Load data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f11e1b83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from datatools import DBWrapper, LoadDataToCassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e3839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cassandra configurations\n",
    "cassandra_cluster_ips = [\"192.168.137.101\"]\n",
    "keyspace = \"events_by_users\"\n",
    "table_name = \"logs_all_events\"\n",
    "\n",
    "\n",
    "# CSV info and configuration\n",
    "files = [\"2019-Dec.csv\"]  # [\"2019-Oct.csv\", \"2019-Nov.csv\", \"2019-Dec.csv\"]\n",
    "path = \"E:/coding/input/ecommerce-behavior-data-from-multi-category-store/\"\n",
    "DTYPE = {\n",
    "    \"event_time\": str,\n",
    "    \"event_type\": \"category\",\n",
    "    \"product_id\": str,\n",
    "    \"category_id\": str,\n",
    "    \"category_code\": str,\n",
    "    \"brand\": str,\n",
    "    \"price\": float,\n",
    "    \"user_id\": str,\n",
    "    \"user_session\": str,\n",
    "}\n",
    "\n",
    "# Size of batch when inserting to Cassandra = number of records for each batch.\n",
    "BATCH_SIZE = 10000\n",
    "# Due to large size of the CSV file, we load rows by chunk.\n",
    "CHUNK_SIZE = 10**6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f71c9d65",
   "metadata": {},
   "source": [
    "## Processing Data and Importing to Cassandra\n",
    "\n",
    "Depending on the size of the batch (more BATCH_SIZE meaning heavier batch query), we need to change the threshold for for batch processing in cassandra.yaml\n",
    "\n",
    "```\n",
    "> batch_size_warn_threshold: 10240KiB\n",
    "> batch_size_fail_threshold: 50240KiB\n",
    "```\n",
    "\n",
    "To copy the `cassandra.yaml` from a container in Docker, we can use the following commands\n",
    "\n",
    "```bash\n",
    "$ docker cp cassandra_node0:/etc/cassandra/cassandra.yaml ~/local/\n",
    "$ nano ~/local/cassandra.yaml\n",
    "$ docker cp ~/local/cassandra.yaml cassandra_node0:/etc/cassandra\n",
    "```\n",
    "\n",
    "In this setup, it took about 4.5 hours (270 minutes) to load in to the cassandra for 68 million records (rate ~ 4000 records/second). The csv file was 9 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7fe5b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 192.168.137.101:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 192.168.137.101:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 0\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 1000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 2000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 3000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 4000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 5000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 6000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 7000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 8000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 9000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 10000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 11000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 12000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 13000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 14000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 15000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 16000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 17000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 18000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 19000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 20000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 21000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 22000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 23000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 24000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 25000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 26000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 27000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 28000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 29000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 30000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 31000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 32000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 33000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 34000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 35000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 36000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 37000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 38000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 39000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 40000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 41000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 42000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 43000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 44000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 45000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 46000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 47000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 48000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 49000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 50000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 51000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 52000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 53000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 54000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 55000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 56000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 57000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 58000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 59000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 60000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 61000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 62000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 63000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 64000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 65000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 66000000\n",
      "DEBUG:datatools:E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv Current # of rows = 67000000\n",
      "DEBUG:datatools: Finished Inserting into Cassandra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time =  16282.749917507172\n"
     ]
    }
   ],
   "source": [
    "info = {\n",
    "    \"ips\": cassandra_cluster_ips,\n",
    "    \"keyspace\": keyspace,\n",
    "    \"table_name\": table_name,\n",
    "}\n",
    "cassandra_conn = DBWrapper(\"cassandra\", info)\n",
    "\n",
    "# if the program stop in the middle of the process, set SKIP_ROWS to continue\n",
    "SKIP_ROWS = None  # None\n",
    "loader = LoadDataToCassandra()\n",
    "\n",
    "for name in files:\n",
    "    file_path = Path(path + name)\n",
    "    print(file_path)\n",
    "    start = time.time()\n",
    "    loader.save_to_cassandra(\n",
    "        cassandra_conn,\n",
    "        file_path,\n",
    "        dtype=DTYPE,\n",
    "        BATCH_SIZE=BATCH_SIZE,\n",
    "        CHUNK_SIZE=CHUNK_SIZE,\n",
    "        SKIP_ROWS=SKIP_ROWS,\n",
    "    )\n",
    "    print(\"Running time = \", time.time() - start)\n",
    "\n",
    "cassandra_conn.disconnect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cdd4bcb",
   "metadata": {},
   "source": [
    "## Process Data\n",
    "\n",
    "We can clean up and fill any missing data with default values in a CSV file and save the processed data into a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c61764f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\coding\\input\\ecommerce-behavior-data-from-multi-category-store\\2019-Dec.csv\n",
      "Running time =  428.40974593162537\n"
     ]
    }
   ],
   "source": [
    "from datatools import LoadDataFromCSV\n",
    "\n",
    "SKIP_ROWS = None  # None\n",
    "loader = LoadDataFromCSV()\n",
    "\n",
    "for name in files:\n",
    "    file_path = Path(path + name)\n",
    "    file_path_new = Path(path + \"cleaned_\" + name)\n",
    "    print(file_path)\n",
    "    start = time.time()\n",
    "    with open(file_path, \"r\") as f:\n",
    "        first_line = f.readline()\n",
    "        with open(file_path_new, \"w\") as f_new:\n",
    "            f_new.write(first_line)\n",
    "\n",
    "    for df in loader.load_by_chunk(\n",
    "        file_path, dtype=DTYPE, chunksize=CHUNK_SIZE, skiprows=SKIP_ROWS\n",
    "    ):\n",
    "        df.to_csv(file_path_new, index=False, header=False, mode=\"a\")\n",
    "\n",
    "    print(\"Running time = \", time.time() - start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5c2aeaf",
   "metadata": {},
   "source": [
    "## Import CSV files: Other Methods\n",
    "\n",
    "If we have well-formed CSV files without missing data or wrong formatting, we can directly import to Cassandra. If the CSV file needs to be processed, we can pre-process the CSV file using Pandas, and save it to a new CSV file.\n",
    "\n",
    "1. `COPY` function from `cqlsh`\n",
    "\n",
    "Besides using Pandas in Python, we can also import CSV files directly using the `COPY` function from `cqlsh`. For example:\n",
    "\n",
    "```\n",
    "> COPY keyspace.table_name (col1, col2, col3) FROM '~/data.csv' WITH HEADER=TRUE ;\n",
    "```\n",
    "\n",
    "Read more here: https://docs.datastax.com/en/cql-oss/3.x/cql/cql_reference/cqlshCopy.html\n",
    "\n",
    "\n",
    "\n",
    "2. DataStax Bulk Loader\n",
    "\n",
    "DataStax\n",
    "\n",
    "```\n",
    "$ dsbulk load -url ~/data.csv -k keyspace -t table_name\n",
    "```\n",
    "\n",
    "Load data from `url`, which could be HTTP link or local file, into `keyspace`.`table_name`\n",
    "\n",
    "More detals here:\n",
    "\n",
    "* https://downloads.datastax.com/#bulk-loader\n",
    "* https://docs.datastax.com/en/dsbulk/docs/reference/dsbulkCmd.html\n",
    "* https://www.datastax.com/blog/datastax-bulk-loader-introduction-and-loading\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cb1a3cd",
   "metadata": {},
   "source": [
    "## `COPY` function from `cqlsh`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d59b151d",
   "metadata": {},
   "source": [
    "In bash shell (`docker exec -it cassandra_node0 bash`)\n",
    "\n",
    "```bash\n",
    "$ cqlsh\n",
    "$ cqlsh> COPY events_by_users.demo_table (event_time, event_type, product_id, category_id, category_code, brand, price, user_id, user_session) FROM '/dir/cassandra/cleaned_2019-Nov.csv'  WITH HEADER=TRUE ;\n",
    "\n",
    "```\n",
    "\n",
    "During my testing run, the rate was about 5000 records per second. Although there were some possible problems:\n",
    "\n",
    "1. `WriteTimeout` error occurred. If the number of records is large, it can cause over load issues and the data might be lost. The `COPY` command should be used with small datasets, such as the number of records is less than millions.\n",
    "\n",
    "2. Can not handle the errors during the importing process. For example, the values are missing, the values are in wrong format (string instead of integer data type). Or even the cases of corupted files due to transportation, for example, moving between servers via networks or copying files to disk or usb.\n",
    "\n",
    "3. Hard to stop or resume the importing process.\n",
    "\n",
    "4. By batching, the capability of servers could be different to handle throughput. If the number of records is large, it can reduce the throughput greatly. The optimal batch size might need to be investigated.\n",
    "\n",
    "Bellow is the error I encountered, the cluster was frozen.\n",
    "\n",
    "```bash\n",
    "Exceeded maximum number of insert errors 1000\n",
    "Failed to process 1005 rows; failed rows written to import_events_by_users_demo_table.err\n",
    "Exceeded maximum number of insert errors 1000\n",
    "Processed: 41925000 rows; Rate:   16906 rows/s; Avg. rate:    4767 rows/s\n",
    "41923995 rows imported from 0 files in 0 day, 2 hours, 26 minutes, and 33.924 seconds (0 skipped).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492012cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
